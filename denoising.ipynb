{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prologue\n",
    "An attempt at implementing [Denoising Induction Motor Sounds](https://arxiv.org/pdf/2208.04462.pdf) for this use case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "\n",
    "import sys, time, random, glob, os, pandas\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pydub\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "from pydub import effects\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "\n",
    "from tensorflow.keras import layers, losses\n",
    "\n",
    "import keras.models\n",
    "from keras.losses import mse as kmse\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Add, Multiply, Lambda, UpSampling2D, Dot, Permute, RepeatVector\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Cropping2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping, ModelCheckpoint\n",
    "import noising\n",
    "from common import *\n",
    "from common import cal_midpoints, gen_mel_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = 'samples'\n",
    "files = glob.glob(f'{INPUT_DIR}/*.flac')\n",
    "noising.save_overlaid_dataset(files, 3, 15, 'dataset.json', 'overlaid', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, directory : str = 'samples', \n",
    "                train_ratio : float = 0.8, window_size : int = 49000,\n",
    "                batch_size : int = 128, gen_type = 'train', shuffle : bool = True):\n",
    "        self.shuffle = shuffle\n",
    "        self.gen_type = gen_type\n",
    "        self.train_ratio = train_ratio\n",
    "        self.test_ratio = self.train_ratio + (1-train_ratio)/2\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = 0\n",
    "        self.window_size = window_size\n",
    "        self.noise_types = ['white', 'pink', 'blue', 'brown', 'violet']\n",
    "        files = glob.glob(f'{directory}/*.flac')\n",
    "        idx = int(train_ratio*len(files))\n",
    "        test_idx = int(self.test_ratio*len(files))\n",
    "        if gen_type == 'train':\n",
    "            self.files = files[0:idx]\n",
    "        elif gen_type == 'test':\n",
    "            self.files = files[idx:test_idx]\n",
    "        else:\n",
    "            self.files = files[test_idx:]\n",
    "        print(f\"{gen_type} loader created with {len(self.files)} samples\")\n",
    "        \n",
    "        self.len = self.__len__()\n",
    "        if self.gen_type != 'test':\n",
    "            self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Denotes the number of batches per epoch'''\n",
    "        return (2*len(self.files)) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle and self.gen_type != 'test' :\n",
    "            random.shuffle(self.files)\n",
    "        self.epoch += 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #! generates self.samples_per_file noisy samples per file, can be of the\n",
    "        #! same kind of noise, or can be of different kinds of noises.\n",
    "        inp = []\n",
    "        out = []\n",
    "        const = 1 #! const files make up a batch in case of 128\n",
    "        for path in self.files[index*const : (index+1)*const]:\n",
    "            clear, noisy = noising.chop_overlay_spit(path, self.window_size, self.noise_types)\n",
    "            inp += noisy\n",
    "            out += clear\n",
    "        inp = np.array(inp)\n",
    "        out = np.array(out)\n",
    "        return inp, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, directory : str = '/home/juggernautjha/Desktop/to_rahul/maruti/data/true_samples', \n",
    "                length : int = 200, train_ratio : float = 0.8,\n",
    "                batch_size : int = 32, gen_type = 'train',\n",
    "                samples_per_file : int = 3, shuffle : bool = True):\n",
    "        self.shuffle = shuffle\n",
    "        self.gen_type = gen_type\n",
    "        self.train_ratio = train_ratio\n",
    "        self.test_ratio = self.train_ratio + (1-train_ratio)/2\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = 0\n",
    "        self.pad_to = length\n",
    "        self.samples_per_file = samples_per_file\n",
    "        self.noise_types = ['white', 'pink', 'blue', 'brown', 'violet']\n",
    "        files = glob.glob(f\"{directory}/*.flac\")\n",
    "        idx = int(train_ratio*len(files))\n",
    "        test_idx = int(self.test_ratio*len(files))\n",
    "        if gen_type == 'train':\n",
    "            self.files = files[0:idx]\n",
    "        elif gen_type == 'test':\n",
    "            self.files = files[idx:test_idx]\n",
    "        else:\n",
    "            self.files = files[test_idx:]\n",
    "        print(f\"{gen_type} loader created with {len(self.files)} samples\")\n",
    "\n",
    "        self.len = self.__len__()\n",
    "        if self.gen_type != 'test':\n",
    "            self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Denotes the number of batches per epoch'''\n",
    "        return (2*len(self.files)) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle and self.gen_type != 'test' :\n",
    "            random.shuffle(self.files)\n",
    "        self.epoch += 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #! generates self.samples_per_file noisy samples per file, can be of the\n",
    "        #! same kind of noise, or can be of different kinds of noises.\n",
    "        inp = []\n",
    "        out = []\n",
    "        const = int(self.batch_size//self.samples_per_file) #! const files make up a batch\n",
    "        for path in self.files[index*const : (index+1)*const]:\n",
    "            colors = random.choices(self.noise_types, k =self.samples_per_file)\n",
    "            for color in colors:\n",
    "                noisy, clear = noising.overlay_noise(path, 15, color, self.pad_to)\n",
    "                inp.append(noisy.get_array_of_samples())\n",
    "                out.append(clear.get_array_of_samples())\n",
    "        inp = np.array(inp)\n",
    "        out = np.array(out)\n",
    "        return inp, out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader created with 16 samples\n",
      "val loader created with 2 samples\n"
     ]
    }
   ],
   "source": [
    "train_generator = FastGenerator()\n",
    "val_generator = FastGenerator(gen_type='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae1a2ab55c544b7bd83eed3db477297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "49000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_generator.__getitem__(1)[0][1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MODEL (finally??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoise(Model):\n",
    "  def __init__(self, input_size, batch_size):\n",
    "    super(Denoise, self).__init__()\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Input(shape = (input_size, 1), batch_size = batch_size),\n",
    "      layers.Conv1D(128, kernel_size=3, activation='relu', kernel_initializer='he_uniform'),\n",
    "      layers.Conv1D(32, kernel_size=3, activation='relu', kernel_initializer='he_uniform'),\n",
    "      layers.Conv1D(16, kernel_size=3, activation='relu', kernel_initializer='he_uniform'),\n",
    "      layers.Conv1D(8, kernel_size=3, activation='relu', kernel_initializer='he_uniform')\n",
    "      ])\n",
    "\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Conv1DTranspose(8, kernel_size=3, activation='relu', kernel_initializer='he_uniform'),\n",
    "      layers.Conv1DTranspose(16, kernel_size=3, activation='relu', kernel_initializer='he_uniform'),\n",
    "      layers.Conv1DTranspose(32, kernel_size=3, activation='relu', kernel_initializer='he_uniform'),\n",
    "      layers.Conv1DTranspose(128, kernel_size=3, activation='relu', kernel_initializer='he_uniform'),\n",
    "      layers.Conv1DTranspose(1, kernel_size=3, activation='relu', kernel_initializer='he_uniform')\n",
    "      ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 01:50:43.250927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-24 01:50:43.350105: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "denoiser = Denoise(49000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoiser.compile('adam', loss=tf.keras.losses.mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55228212b7f04163a5257dfedfd8bb3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 01:51:22.017169: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25088000 exceeds 10% of free system memory.\n",
      "2023-06-24 01:51:22.036622: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 50176000 exceeds 10% of free system memory.\n",
      "2023-06-24 01:51:22.083972: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 3211132928 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "denoiser.fit(train_generator, validation_data=val_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
